## Data and Notebook Instructions

# Data

Included within our resources is a 'datasets.zip' file, which contains a selection of Darwin datasets https://github.com/MasterAI-EAM/Darwin/tree/main/dataset. For convenience, you can directly upload this zip file using the following command and then unzip it to access the datasets. Alternatively, if you prefer to use a different dataset, feel free to select any of your choice from the Darwin project [repository](https://github.com/MasterAI-EAM/Darwin/tree/main/dataset) at Darwin Dataset Collection. Once selected, compress your chosen dataset and upload it for use.

# Notebooks

LLM_Finetune_For_Solubility_Data.ipynb

This notebook is an adaptation of the Darwin training script, available at Darwin Training Script. It has undergone thorough testing and is confirmed to yield reliable outcomes for solubility data fine-tuning.

LLM_Finetune_For_Solubility_Data_PEFT.ipynb

This notebook introduces the concept of Parameter Efficient Fine Tuning (PEFT), a cutting-edge approach designed to enhance the fine-tuning process of large language models (LLMs) for specific datasets. It represents an advanced technique for optimizing model performance with minimal adjustments to the model parameters.

# Getting Started

Data Upload and Preparation:

Upload the 'datasets.zip' file and unzip it to access the packaged Darwin datasets.
For custom datasets, download from the provided Darwin link, compress, and then upload.

Notebook Execution:

Begin with the LLM_Finetune_For_Solubility_Data.ipynb notebook to implement the standard Darwin training approach.
Proceed to the LLM_Finetune_For_Solubility_Data_PEFT.ipynb for exploring Parameter Efficient Fine Tuning.




 
